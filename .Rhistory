paste("simlist$naiveProp", names(simlist$naiveProp),
"distance", "'unifrac'", "weighted=TRUE", sep=comdelim),
paste("simlist$naiveProp", names(simlist$naiveProp),
"distance", "'bray'", sep=comdelim))
distcommands = c(distcommands,
paste("simlist$naiveProp", names(simlist$naiveProp),
"AitchDistphyloseq", sep=comdelim)
)
# rarefy - random subsample
distcommands = c(distcommands,
paste("simlist$rarefy", names(simlist$rarefy),
"distance", "'unifrac'", sep=comdelim),
paste("simlist$rarefy", names(simlist$rarefy),
"distance", "'unifrac'", "weighted=TRUE", sep=comdelim),
paste("simlist$rarefy", names(simlist$rarefy), "distance", "'bray'", sep=comdelim)
)
# edgeR normalized
# Define groups of normalizations/parameters to use
edgeRnormtypes = c("simlist$edgeRTMM", "simlist$edgeRRLE",
"simlist$edgeRupqua", #"simlist$edgeRnone",
"simlist$rarefy")
edgeRmethtypes = c("method='logFC'", "method='bcv'")
#edgeRgenetypes = c("gene.selection='pairwise'", "gene.selection='common'")
edgeRgenetypes = c("gene.selection='pairwise'")
edgeRtopvalues = c("top=2000")
edgeRmatcoms = expand.grid(edgeRnormtypes, names(simlist$edgeRTMM), "edgeRdist",
edgeRmethtypes, edgeRgenetypes, edgeRtopvalues)
edgedistcoms = apply(edgeRmatcoms, 1, paste0, collapse=comdelim)
distcommands = c(distcommands, edgedistcoms)
# DESeq. Using DESeq directly, not via PoiClaClu
# Define groups of normalizations/parameters to expand
#### "simlist$DESeqFit", "simlist$DESeqGene")
DESeqnormtypes = c("simlist$none", "simlist$DESeqVS", "simlist$rarefy")
DESeqdisttypes = c("method='euclidean'", "method='unifrac', weighted=TRUE", "method='bray'")
DESeqmatcoms = expand.grid(DESeqnormtypes, names(simlist$DESeqVS),
"distance", DESeqdisttypes)
DESeqdistcoms = apply(DESeqmatcoms, 1, paste0, collapse=comdelim)
distcommands = c(distcommands, DESeqdistcoms)
# PoiClaClu "normalized"... built-in to the model used in the distance function:
PoiClaCluNorms = c("simlist$none", "simlist$rarefy")
PoiClaCluDists = c("type='mle'", "type='deseq'", "type='quantile'")
PoiClaClumatcoms = expand.grid(PoiClaCluNorms, names(simlist$none),
"PoissonDist", PoiClaCluDists)
PoiClaCludistcoms = apply(PoiClaClumatcoms, 1, paste0, collapse=comdelim)
distcommands = c(distcommands, PoiClaCludistcoms)
#' Calculate distances using the vector of `"_"`-delimited commands above.
require("foreach")
dlist <- foreach(dcom=distcommands, ## pois
.packages=c("phyloseq", "PoiClaClu", "edgeR", "compositions", "usedist", "CompSign"),
.export=c("simlist", "edgeRnorm", "edgeRdist", "PoissonDist", "AitchDistphyloseq", "deseq_varstab")) %dopar% {
cmdsplit = strsplit(dcom, split=comdelim, fixed=TRUE)[[1]]
physeq = eval(parse(text=cmdsplit[1]))[[paste0(cmdsplit[2:4], collapse=comdelim)]]
if(length(cmdsplit) == 5){
if(grepl('Aitch', cmdsplit[5])){
dd = eval(parse(text= paste0(cmdsplit[5], "(physeq)")))
}else{
dd = do.call(cmdsplit[5], physeq)
}
} else if(length(cmdsplit) > 5){
fcall = paste0(cmdsplit[5], "(physeq, ",
paste(cmdsplit[6:length(cmdsplit)], collapse=", "),
")")
dd = eval(parse(text=fcall))
} else {
stop("split distance command string is unexpected length")
}
# Check that dd has Labels, or it will screw up clustering eval downstream
if( is.null(attributes(dd)$Labels) ){
attributes(dd)$Labels <- sample_names(physeq)
}
return(dd)
}
names(dlist) <- distcommands
all(sapply(dlist, class) == "dist")
# distance(physeq, 'AitchDistphyloseq')
# AitchDistphyloseq(physeq)
# distance(physeq, 'PoissonDist')
#' `NA` distances. So far, it looks like it is very rare for any
#' distance matrices to have `NA` values, but if it does happen it
#' makes `pam` fail. The reason for this happening is unclear right
#' now, but appears constrained to the `PoissonDistance` results.
#' My first guess is that these simulated samples have nothing in
#' common, and so `PoissonDistance` gives them a value of `NA` rather
#' than infinity (or 1, if it is normalized distance). Instead of
#' removing these distance matrices (which is one option for this
#' simulation), since this is a rare event, I will instead give these
#' entries the max value that otherwise occurred in the distance matrix.
# Define function to
# Replace infinite or NA values in distance matrices
# with 1.25x the max value
fixbadDs = function(dlist){
lapply(dlist, function(dd){
mx = max(dd, na.rm=TRUE)
mx = 1.25 * mx
if( any(is.infinite(dd)) ){
dd[is.infinite(dd)] <- mx
}
if( any(is.na(dd)) ){
dd[is.na(dd)] <- mx
}
return(dd)
})
}
dlist = fixbadDs(dlist)
#' Binary clustering using `limma::pam` (partitioning around medoids)
#' with `k==2`, and alternatively `stats::kmeans` and `stats::hclust`.
# pam
CLlist <- foreach(dd=dlist, .packages='cluster') %dopar% {
pam(dd, k=2, cluster.only=TRUE)
}
names(CLlist) <- names(dlist)
# kmeans
CLlist_kmeans <- foreach(dd=dlist, .packages='stats') %dopar% {
kmeans(cmdscale(dd, 2), 2)$cluster
}
names(CLlist_kmeans) <- names(dlist)
# hclust
CLlist_hclust <- foreach(dd=dlist, .packages='stats') %dopar% {
cutree(hclust(dd), k=2)
}
names(CLlist_hclust) <- names(dlist)
#' Evaluate the results of each clustering.
# Evaluate each clustering result in CLlist
binaryclusteval = function(x, tot){
# Assumes argument, x, is a named cluster vector with two components,
# and that the "true" clustering can be inferred from the name of each
# element, namely, the non-digit, non-delimiter word in each name.
# x = CLlist[[1]]
# tot = 2*J
#truth = as(factor(gsub("[[:digit:]]{1}\\:\\:[[:digit:]]{1,}", "", names(x))), "numeric")
truth = as(factor(gsub("[[:print:]]+\\:\\:[[:digit:]]{1,}", "", names(x))), "numeric")
xinv = ifelse(x==1, 2, 1)
correct = max( sum(x==truth), sum(xinv==truth))
return(
c(fracCorrectPred=(correct/length(x)), fracCorrect=(correct/tot))
)
}
edf = ldply(CLlist, function(x, binaryclusteval, tot){
clres = binaryclusteval(x, tot)
return(
data.frame(fracCorrectPred=clres["fracCorrectPred"], fracCorrect=clres["fracCorrect"])
)
}, binaryclusteval, tot=(2*J))
# Add the commands to the first column of edf, with name `fcall`
edf = data.frame(fcall=names(CLlist), edf, stringsAsFactors=FALSE)
#colnames(edf)[1] <- "fcall"
#' Now parse the clustering results table, `edf`.
# Define the normalization type.
edf$normtype <- gsub("(simlist\\$)([[:alnum:]]+)(\\_[[:print:]]+$)", "\\2", edf$fcall)
# Define the expected number of reads included.
edf$nreads <- as(sapply(strsplit(edf$fcall, comdelim, fixed=TRUE), function(x) x[2]), "integer")
# The replicate number
edf$replicate <- as(sapply(strsplit(edf$fcall, comdelim, fixed=TRUE),
function(x) x[3]), "numeric")
# The mix factor
edf$mix_factor <- as(sapply(strsplit(edf$fcall, comdelim, fixed=TRUE),
function(x) x[4]), "numeric")
# use the remaining info to define the function call
edf$distmethod <- sapply(strsplit(edf$fcall, comdelim, fixed=TRUE),
function(x) paste(x[-(1:4)], collapse=", "))
# Melt on fractionPredicted/fractionTotal so that
require("reshape2")
edf = melt(edf, measure.vars=c("fracCorrectPred", "fracCorrect"))
colnames(edf)[colnames(edf) == "variable"] <- "AccuracyMeasure"
colnames(edf)[colnames(edf) == "value"] <- "Accuracy"
colnames(edf)[colnames(edf) == "normtype"] <- "Normalization"
# Relabel for plotting
colnames(edf)[colnames(edf)=="distmethod"] <- "Distance"
colnames(edf)[colnames(edf)=="mix_factor"] <- "Effect_Size"
# Relabel Distance measure names
edf$Distance <- gsub("distance, 'unifrac', weighted=TRUE", "UniFrac-w", edf$Distance, fixed=TRUE)
edf$Distance <- gsub("distance, method='unifrac', weighted=TRUE", "UniFrac-w", edf$Distance, fixed=TRUE)
edf$Distance <- gsub("distance, method='euclidean'", "Euclidean", edf$Distance, fixed=TRUE)
edf$Distance[grep("distance, 'unifrac'", edf$Distance)] <- "UniFrac-u"
edf$Distance[grep("edgeRdist", edf$Distance)] <- "top-MSD"
edf$Distance[grep("bray", edf$Distance)] <- "Bray-Curtis"
edf$Distance[grep("PoissonDist", edf$Distance)] <- "PoissonDist"
# Relabel normalization methods
edf$Normalization[edf$Normalization=="naiveProp"] <- "Proportion"
edf$Normalization[edf$Normalization=="edgeRupqua"] <- "UQ-logFC"
edf$Normalization[edf$Normalization=="rarefy"] <- "Rarefy"
edf$Normalization[edf$Normalization=="none"] <- "None"
# Add a distance options variable, Options
edf$DistCall <- sapply(strsplit(edf$fcall, comdelim, fixed=TRUE),
function(x) paste(x[-(1:4)], collapse=", "))
edf$Options <- sapply(strsplit(edf$fcall, comdelim, fixed=TRUE),
function(x) paste(x[-(1:5)], collapse=", "))
#' Create main plot. Summarize clustering accuracy performance
#' at different effect sizes, library sizes using different
#' normalization and distance measures. Try comparing across
#' methods and down library size before any subsetting of the
#' results. Select the best subset of parameters to capture the
#' behavior of the different normalization/distance methods in
#' a graphic size that can fit as a main figure. The complete
#' results will be shown in the supplemental version.
edfmain = subset(edf, AccuracyMeasure=="fracCorrect")
edfmain = subset(edfmain, nreads %in% c(1000, 5000))
edfmain = subset(edfmain, !Distance %in% c("UniFrac-u"))
# Subset to the best for each panel (Distance, nreads, Normalization)
edfmain = ddply(edfmain, c("Distance", "nreads", "Normalization"), function(df){
panelmeans = daply(df, "Options", function(x){mean(x$Accuracy)})
if( length(panelmeans) > 1){
df = df[df$Options == names(panelmeans[which.max(panelmeans)]), ]
}
# else leave as-is
return(df)
})
#' Graphically summarize error between different
#' normalization/distance types using the same clustering method.
#### Main figure clustering
#' Calculate means, store as separate `dfmean` table.
nonrepvars = c("nreads", "Effect_Size", "Distance", "Normalization")
edfmean = ddply(subset(edf, AccuracyMeasure=="fracCorrect"),
.variables=nonrepvars, .fun=function(x, vars){
data.frame(x[1, vars, drop=FALSE], Accuracy=mean(x$Accuracy), sd.Acc=sd(x$Accuracy), stringsAsFactors=FALSE)
}, nonrepvars)
# Subset to just the panels you want to show in the figure.
edfsmean = subset(edfmean,
!Normalization %in% c("edgeRTMM", "edgeRRLE") &
!Distance %in% c("PoissonDist, type='mle'", "PoissonDist, type='deseq'") &
!nreads %in% c(5000))
edfsmean$Distance[edfsmean$Distance == "PoissonDist, type='quantile'"] <- "PoissonDist"
## Figure 3
#'  Further refine data in `edfsmean` for better plotting.
# Rename the library size to include plotmath symbol for clarity
lib_levels = paste0("tilde(N)[L]==", sort(unique(as.integer(edfsmean$nreads))))
edfsmean$nreads <- factor(paste0("tilde(N)[L]==", edfsmean$nreads), levels=lib_levels)
pathsize = 0.5
pointsize = 1.5
errorwidth = 0.1
erroralpha = 0.35
errorsize = 0.4
normlegtitle = "Normalization Method:"
pclust0 = ggplot(edfsmean, aes(Effect_Size, Accuracy, color=Normalization)) +
geom_errorbar(aes(ymax=Accuracy+sd.Acc, ymin=Accuracy-sd.Acc), width=errorwidth, alpha=erroralpha, size=errorsize, position="dodge") +
geom_path(size=pathsize) +
geom_point(aes(shape=Normalization), size=pointsize) +
facet_grid(nreads ~ Distance, labeller=label_parsed) +
scale_colour_discrete(guide=guide_legend(title=normlegtitle)) +
scale_shape_discrete(guide=guide_legend(title=normlegtitle)) +
scale_y_continuous(breaks=c(0.6, 0.8, 1.0), limits=c(0.5, 1.0), oob=function(x,limits){x}) +
scale_x_continuous(breaks=c(1.15, 2.0, 3.0), minor_breaks=c(1.5, 2.5, 3.5)) +
xlab("Effect Size") +
theme(plot.margin=unit(c(0, 0, 0, 0), "cm")) +
theme(text=element_text(size=8), plot.title=element_text(size=10), strip.text=element_text(size=8)) +
theme(legend.position="top", legend.box="horizontal", legend.margin=unit(-0.5, "cm")) +
theme(strip.background=element_blank()) # + ggtitle("Clustering Accuracy")
print(pclust0)
library(CompSign)
library(CompSign)
aaply
??aaply
aaply
simlist
#' (FM) This paper by Susan Holmes has a simulation strategy
#' that we should adopt. Where they use mibrobiome data,
#' we use signatures from PCAWG (for SNVs) or TCGA (for CNs).
#'
#' (paper) Observed samples are generated through multinomial
#' sampling.
#' we created two microbiome simulation workflows based on
#' repeated subsampling from empirical data.
#' The way I see it, this might be relevant for discrete mutational
#' categories such as those of SNPs, but not for Copy Number.
#' - false, we just have to sample from a dirichlet and not from a
#' multinomial
#'
#' In CN we should have dirichlet sampling instead of multionomial sampling
rm(list = ls())
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
source("../../functions/basic_functions.R", print.eval = TRUE)
reqpkg = c("edgeR", "PoiClaClu", "phyloseq", "DESeq",
"foreach", "doParallel",
"plyr", "reshape2", "ggplot2", "grid", "scales", "cluster",
"fields", "ComplexHeatmap", "circlize", "ape")
inpkg = installed.packages()[, "Package"]
neededpkg = reqpkg[!reqpkg %in% inpkg]
if(length(neededpkg) > 0){
stop(paste("Need to install the following package:", neededpkg))
}
sapply(reqpkg, library, character.only = TRUE)
source("holmes_et_al_scripts/holmes_et_al_funs.R", print.eval = TRUE)
signature_type <- "emu" # "original_holmes"   # "emu" # "COSMIC"
myversion <- TRUE
recompute_exposures <- FALSE
## Redo or load assigned mutations
if(recompute_exposures){
##Â run code in holmes_et_al_datasets
}else{
if(signature_type == "COSMIC"){
cancertype1 <- 'Breast'
cancertype2 <- 'Melanoma'
load(paste0("../../../data/Robj/DirichletMultinomials/SHolmesSim/YAPSA_", cancertype1, "_", cancertype2, '/image.Rdata'))
}else if(signature_type == "emu"){
cancertype1 <- 'Breast'
cancertype2 <- 'Melanoma'
load(paste0("../../../data/Robj/DirichletMultinomials/SHolmesSim/EMu_", cancertype1, "_", cancertype2, '/image.Rdata'))
}else if(signature_type == "original_holmes"){
cancertype1 <- 'Feces'
cancertype2 <- 'Ocean'
load(paste0("../../../data/Robj/DirichletMultinomials/SHolmesSim/Holmes_", cancertype1, "_", cancertype2, '/image.Rdata'))
}
}
stopifnot(nrow(Group1_orig) == nrow(Group2_orig))
nSigs <- nrow(Group1_orig)
#' note: there is a data object Group2 {MASS} which
#' might be loaded instead if not careful
## equivalence
## medium (Group1/Group2)   cancer type (Group1/Group2)
## OTU                        exposures to mutational signatures
## samples                    samples
# pdf("../../../results/DirichletApproach/SHolmes/Group1_vs_Group2.pdf", width = 10, height = 8)
# par(mfrow=c(6,2))
# for(r in 1:6){
#   tmp_sim <- createSim(ntop = 30)
#   # image(tmp_sim[['Group1']], main='Sim Group1')
#   # image(tmp_sim[['Group2']], main='Sim Group2')
#   h1 <- ComplexHeatmap::Heatmap(tmp_sim[["Group1"]], column_title = 'Simulation Group1', cluster_rows = FALSE)
#   h2 <- ComplexHeatmap::Heatmap(tmp_sim[["Group2"]], column_title = 'Simulation Group2', cluster_rows = FALSE)
#   draw(h1+h2)
# }
# dev.off()
#' what is our error like? is it negative binomial?
#' (how on earth can I know that?)
par(mfrow=c(1,1), mar=c(3,3,3,3))
max_counts_cat_Group1 <- Group1_orig[which.max(rowSums(Group1_orig)),]
hist(max_counts_cat_Group1, breaks=100)
hist(max_counts_cat_Group1[max_counts_cat_Group1>0], breaks=100)
## differential expression analysis
#' we are comparing the counts for each mutational signature,
#' for different types of cancer
boxplot((Group1_orig[,3]/Group1_orig[,4]),
(Group2_orig[,3]/Group2_orig[,4]))
for(i in 1:5){
print(t.test(Group1_orig[,i], Group2_orig[,i])$p.value)
}
matrix_both <- cbind(Group1_orig, Group2_orig)
colnames(matrix_both) <- NULL #as.character(1:ncol(matrix_both))
# Heatmap(matrix_both, cluster_columns = FALSE,
#         top_annotation = HeatmapAnnotation(df = data.frame(type=c(rep('Group1', ncol(Group1_orig)),
#                                                                   rep('Group2', ncol(Group2_orig))))))
###############################################
############# Clustering accuracy #############
###############################################
theme_set(theme_bw())
pal = "Set1"
scale_colour_discrete <-  function(palname=pal, ...){
scale_colour_brewer(palette=palname, ...)
}
scale_fill_discrete <-  function(palname=pal, ...){
scale_fill_brewer(palette=palname, ...)
}
## Set parameters for this simulation
set.seed(20140206)
#savedatafile = "simulation-cluster-accuracy"
if(myversion){
sampletypes = c('Group1 cancer', 'Group2')
}else{
sampletypes = c("Feces", "Ocean")
}
# Number of OTUs in simulation.
# This is the maximum.
# For less-diverse template, or simulated samples with fewer reads,
# the actual number of OTUs will be much less.
if(myversion){
nOTUs = 30L ## we only have 5 signatures
}else{
nOTUs = 20L
}
#nOTUs = 2000L
# Minimum number of reads to consider an OTU "observed" in a sample
minobs= 1L
# Samples per simulation
J = 10L
#J = 40L
# Effect Size. The artificial mix fraction.
mixfacs = c(1, 1.15, 1.25, 1.5, 1.75, 2, 2.5, 3.5)
ns = c(1000, 2000, 5000, 1E4)
if(myversion){
ns = c(10, 100) #' got an error saying " Error in sample.int(length(x), size, replace, prob) :
#' cannot take a sample larger than the population when 'replace = FALSE'
}
# Vector of the replicate numbers to repeat for
# each comb of simulation parameters (n, etc)
reps=1:5
# The delimiter for command parameters
comdelim = "_"
# Number of cores. Only relevent for certain types of back-end registration.
Ncores = 1
#Ncores = 8
# rarefying power params
rarefy_fracs = c(0, seq(0.05, 0.25, 0.05), 0.4)
rarereps = 1:2
# The combinations of simulation parameters,
# used later in the script.
simparams = apply(expand.grid(ns, reps, mixfacs), 1, paste0, collapse=comdelim)
# Define date-stamp for file names
datestamp = gsub(":", "_", gsub("[[:space:]]+", "-", date()), fixed=TRUE)
print(datestamp)
## Parameter definitions
##' The major factors contributing to the computation cost in this simulation example are the number of OTUs
##' retained from the template `GlobalPatterns` dataset, which ultimately is used to dictate the length
##' of the multinomials and their corresponding proportion vectors specified by $\pi$ (R variable `pi` or `pis`);
##' and the number of samples being simulated.
#' - `nOTUs` -- the number of most-prevalent OTUs to keep in simulation template, `r nOTUs`.
#' - `minobs` -- The minimum abundance value, `r minobs`, for which an OTU is "counted" as having been observed
#' in a given sample. This is used for ranking OTUs according to the number of samples in which they appeared.
#' - `J` -- The number of samples per simulated table. For multitable analysis this needs to be consistent across
#' tables, so included here in the beginning as a global parameter. Value for this simulation is `r J`.
#' - `mixfacs` -- The fold-weighting of the template multinomial compared with its mixed-in counterpart.
#' Same in both directions (symetric mixing).
#' - `ns` -- A vector of the expected values for the sampling depth that are nevertheless subject to the
#' random sampling from the original template totals. The read numbers should not exceed the total number of reads
#' in the template, so this is checked in the "simulation" (subsampling) module, and a ceiling used. The values for
#' this vector of sampling depth means in this particular simulation is: `r ns`.
# Define template
# Trim to just those samples that are intended as template in this binary effect.
data("GlobalPatterns")
if(myversion){
# signatures (taxa) are rows
GlobalPatterns@otu_table <- otu_table(cbind(Group1_orig, Group2_orig), taxa_are_rows = TRUE)
GlobalPatterns@tax_table <- tax_table(matrix("NA", nSigs, 1, dimnames = list(rownames(Group2_orig), 'dum'))) ## dummy
GlobalPatterns@sam_data <- sample_data(data.frame(row.names = colnames(GlobalPatterns@otu_table), SampleId=colnames(GlobalPatterns@otu_table),
SampleType=c(rep('Group1 cancer', ncol(Group1_orig)), rep('Group2', ncol(Group2_orig)))))
## simulate a phylogeny
tree <- rtree(n = 5); tree$tip.label <- rownames(Group2_orig)
GlobalPatterns@phy_tree <- phy_tree(tree) ## keep as it is
}
sampsums = sample_sums(GlobalPatterns)
keepsamples = sample_data(GlobalPatterns)$SampleType %in% sampletypes
template = prune_samples(keepsamples, GlobalPatterns)
# Trim OTUs that do not appear in very many samples in the template. Sort by prevalance (number of samples appeared) and then by abundance value.
samobs = apply(otu_table(template), 1, function(x, m) sum(x > m), m=minobs)
otudf = data.frame(prev=samobs, sums=taxa_sums(template))
otudf = otudf[order(-otudf$prev, -otudf$sums), ]
# Trim all but the first nOTUs
template = prune_taxa(rownames(otudf)[1:nOTUs], template)
template
# Simulate microbiome census
# In a previous version I used the `simPop` function from [the dirmult package](http://cran.r-project.org/web/packages/dirmult/index.html).
# Instead, here I am randomly subsampling from the original templates at different numbers of total samples.
## Define templates
template1 = subset_samples(template, SampleType==sampletypes[1])
template2 = subset_samples(template, SampleType==sampletypes[2])
## Forced mixing.
#' In the original version 4 result, it appeared that there was very little overlap
#' between the two template types. This could easily be the case, even though they
#' were both body habitats. One biologically intuitive approach would be to choose
#' a different pair of template sample types that are more similar. However, this
#' doesn't afford much additional intuition about the extent that they are similar
#' (without a bunch of extra exploratory analysis), and doesn't give us very much
#' control over the extent that they are similar... we'd be stuck with whatever
#' similarities are "available" among the sample types. Note that it might also help
#' to increase the number of OTUs being included in template and simulation. At the
#' moment it is `r nOTUs`.
#' Instead, we will intentionally mix two templates that otherwise have very few OTUs
#' in common, now labeled `template1` and `template2`, by adding a known but small
#' proportion of each template to the other.
# merge the template components together into one sample.
template1 = merge_samples(template1, "SampleType")
template2 = merge_samples(template2, "SampleType")
## Register parallel backend for computing
#' Register parallel clusters for parallel calculations (e.g. UniFrac, etc.).
# The parallel single-node way
cl <- makeCluster(Ncores)
# Register the parallel cluster
registerDoParallel(cl)
## Simulate microbiome samples from different 'dirty' templates
#' Repeat the simulation many times, with different values for the number of reads per sample.
# Parallelized simulations
simparams_old <- simparams
simparams <- simparams ##otherwise it takes extremely long
#simlist <- lapply(simparams, function(i){#.packages=c("phyloseq")) %dopar% {
simlist <- foreach(i=simparams, .packages=c("phyloseq")) %dopar% {
print(i)
# i = simparams[4]
# Initialize
n = sim = sim1 = sim2 = n1 = n2 = NULL
#cat(i, "\n")
n = as.numeric(strsplit(i, comdelim)[[1]][1])
mixfac = as.numeric(strsplit(i, comdelim)[[1]][3])
# Rarely a simulation has a weird value and fails.
# Catch these with `try`, and repeat the simulation call
# if error (it will be a new seed)
tryAgain = TRUE; infiniteloopcounter = 1
while(tryAgain & infiniteloopcounter < 5 ){
n1   = sumsim(n, sampsums, J)
n2   = sumsim(n, sampsums, J)
sim1 = microbesim(sampletypes[1], template1, template2, mixfac, J, n1) ## here's the error about sampling
sim2 = microbesim(sampletypes[2], template2, template1, mixfac, J, n2)
if( is.null(sim1) | is.null(sim2) |
is.null(n1) | is.null(n2) |
inherits(sim1, "try-error") | inherits(sim2, "try-error")){
tryAgain = TRUE
infiniteloopcounter = infiniteloopcounter + 1
} else {
tryAgain = FALSE
}
}
if( infiniteloopcounter >= 5 ){
return(NULL) ## this I added myself. maybe it should be NA instead?
#  stop("Consistent error found during simulation. Need to investigate cause.")
}
# Merge the two simulated datasets together into one phyloseq object
# and add back tree.
sim = merge_phyloseq(sim1, sim2)
sim = merge_phyloseq(sim, tax_table(GlobalPatterns), phy_tree(GlobalPatterns))
return(sim)
}
names(simlist) <- simparams
simlist
template1
